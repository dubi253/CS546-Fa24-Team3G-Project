## Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines

[üåê[Project Page](https://cnzzx.github.io/VSA/)]
[üìÑ[Paper](https://arxiv.org/abs/2410.21220)]
[ü§ó[Hugging Face Space](https://huggingface.co/spaces/Yiyuan/VSA)]
[üíª[Model Zoo](./docs/model_zoo.md)]
[üìñ[Introduction](https://zhuanlan.zhihu.com/p/4479795939)]
[üé•[Video](https://www.bilibili.com/video/BV1c5DPYmE9s)]

<img src="assets/teaser.png">

## üí´ Release

- [2024/10/29] We released the code for the local demo.
- [2024/10/29] Vision Search Assistant is released on [arxiv](https://arxiv.org/abs/2410.21220). 

## ‚öôÔ∏è Setup

1. Clone this repository and navigate to VSA folder.
```shell
git clone https://github.com/dubi253/CS546-Fa24-Team3G-Project
cd CS546-Fa24-Team3G-Project
```

or use SSH:

```shell
git clone git@github.com:dubi253/CS546-Fa24-Team3G-Project.git
cd CS546-Fa24-Team3G-Project
```

2. Create conda environments.
```
conda create -n CS546 python=3.10
conda activate CS546
```

3. Install LLaVA.
```
cd models/LLaVA
pip install -e .
```

4. Install other requirements.
```
pip install -r requirements.txt
```


## ‚≠ê Local Demo
The local demo is based on gradio, and you can simply run with:

```
python app.py
```

### Run Inference
<img src="assets/inst_01.png">

- In the "Run" UI, you can upload one image in the "Input Image" panel, and type in your question in the "Input Text Prompt" panel. Then, click submit and wait for model inference. 
- You can also customize object classes for detection in the "Ground Classes" panel. Please separate each class by commas (followed by a space), such as "handbag, backpack, suitcase."
- On the right are temporary outputs. "Query Output" shows generated queries for searching, and "Search Output" shows web knowledge related to each object.

### Try with Samples
<img src="assets/inst_02.png">

We provide some samples for you to start with. In the "Samples" UI, you can select one in the "Samples" panel, click "Select This Sample", and you will find sample input has already been filled in the "Run" UI.

## üìü CLI Inference
You can also chat with our Vision Search Assistant in the terminal by running.

```
python cli.py \
    --vlm-model "liuhaotian/llava-v1.6-vicuna-7b" \
    --ground-model "IDEA-Research/grounding-dino-base" \
    --search-model "internlm/internlm2_5-7b-chat" \
    --vlm-load-4bit
```

Then, select an image and type your question.

## üìù License

This project is released under the [Apache 2.0 license](LICENSE).

## Acknowledgements
Vision Search Assistant is greatly inspired by the following outstanding contributions to the open-source community: [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO), [LLaVA](https://github.com/haotian-liu/LLaVA), [MindSearch](https://github.com/InternLM/MindSearch).

## Citation

If you find this project useful in your research, please consider cite:

```
@article{zhang2024visionsearchassistantempower,
  title={Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines},
  author={Zhang, Zhixin and Zhang, Yiyuan and Ding, Xiaohan and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2410.21220},
  year={2024}
}
```
